{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "Train a simple deep nerual netowrk (DNN) to classify between groups of subjects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Heart disease is the leading cause of death globally. __Heart failure__ and __Hypertrophic cardiomyopathy__  are the primary cause of heart diseases, and that refers to a physiological state in which the heart is unable to pump sufficiently to maintain blood flow to meet the body's need. It usually occurs because the heart has become too weak or stiff. \n",
    "\n",
    "HF is routinely diagnosed by:\n",
    "* __Electrocardiogram (ECG)__, which measure heart's rhythm and electrical activity. From the ECG the QRS duration can be computed. An increased QRS duration, indicate dyssynchronous contraction and relaxation of the left and right ventricles and it is a marker of detection of HF subjects.\n",
    "* __MRI__ or __US imaging__ which provides structural and functional information of the heart. Evaluation of the structure and the function of the ventricles can provide useful information for diagnosis and characterization of disease. \n",
    " \t \t\n",
    "Based on the result of these tests, doctors use NYHA class to classify patients' heart failure according to the severity of their symptoms:\n",
    "\n",
    "__Class I__: no limitation is experienced in any activities; there are no symptoms from ordinary activities.\n",
    "\n",
    "__Class II__: slight, mild limitation of activity; the patient is comfortable at rest or with mild exertion.\n",
    "\n",
    "__Class III__: marked limitation of any activity; the patient is comfortable only at rest.\n",
    "\n",
    "__Class IV__: any physical activity brings on discomfort and symptoms occur at rest.\n",
    "\n",
    "### Current diagnostic technique\n",
    "\n",
    "Currently, the parameters used in clinics to identify cardiac patients are:\n",
    "* __LVEDV__ which is the maximum amount of blood that heart can pump\n",
    "* __LVESV__ which is the minium amount of blood that heart can pump\n",
    "* __LVSV__ which is the amount of blood pumped by the left ventricle of the heart in one contraction. \n",
    "* __Ejection fraction (EF)__, which computes the amount of blood of the left ventricle (LV) pumps out with each contraction. A normal heartâ€™s ejection fraction may be between 50 and 70 percent.\n",
    "\n",
    "\n",
    "### Analsysis\n",
    "\n",
    "In this case study we aim develop a DNN network to classify between the healthy, heart failure and hypertrophic cardiomyopathy subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all the import that you will use for this case study\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, yPred, target_names):\n",
    "    \"\"\" Compute metrics\n",
    "    Parameters\n",
    "    ----------\n",
    "    yPred: labels predicted using DNN\n",
    "    y_true: ground truth labels\n",
    "    target_names:  names matching the labels\n",
    "    \"\"\"\n",
    "\n",
    "    BACC = balanced_accuracy_score(y_true, yPred)\n",
    "    PRE = precision_score(y_true, yPred, average=None)\n",
    "    REC = recall_score(y_true, yPred, average=None)\n",
    "    print('Balanced accuracy  {0:.2f}'.format(BACC))\n",
    "\n",
    "    headers = [\"precision\", \"recall\"]\n",
    "\n",
    "    rows = zip(target_names, PRE, REC)\n",
    "\n",
    "    digits = 2\n",
    "    longest_last_line_heading = 'weighted avg'\n",
    "    name_width = max(len(cn) for cn in target_names)\n",
    "    width = max(name_width, len(longest_last_line_heading), digits)\n",
    "    head_fmt = u'{:>{width}s} ' + u' {:>9}' * len(headers)\n",
    "    report = head_fmt.format(u'', *headers, width=width)\n",
    "    report += u'\\n\\n'\n",
    "    row_fmt = u'{:>{width}s} ' + u' {:>9.{digits}f}' + u' {:>9}\\n'\n",
    "    for row in rows:\n",
    "        report += row_fmt.format(*row, width=width, digits=digits)\n",
    "    report += u'\\n'\n",
    "\n",
    "    print(report)\n",
    "\n",
    "    return BACC * 100, PRE * 100, REC * 100\n",
    "\n",
    "def PlotThreeClassData(X,y, title, className):\n",
    "    \"\"\" Plot three class data \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: data\n",
    "    y: labels\n",
    "    title: name of the plot\n",
    "    className: Name of the classes\n",
    "    \"\"\"                 \n",
    "    plt.scatter(X[y==0,0],X[y==0,1],c='r', label = className[0])\n",
    "    plt.scatter(X[y==1,0],X[y==1,1],c='b', label = className[1])\n",
    "    plt.scatter(X[y==2,0],X[y==2,1],c='m', label = className[2])\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "\n",
    "def PlotTwoClassData(X, y, title, _caseTypeNames):\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], c='r', label=_caseTypeNames[0])\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='b', label=_caseTypeNames[1])\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    \n",
    "# Create a DNN network for classification with two layers with sizes 20 and 10. \n",
    "class DNN(torch.nn.Module):\n",
    "    '''Plain dense neural network of linear layers using dropout and ReLU activation.\n",
    "    Size of the hidden layers: [20,10]\n",
    "    numClasses: number of classes used for the classification'''\n",
    "    def __init__(self, numClasses):\n",
    "        super().__init__()\n",
    "        dropout_value = 0.05\n",
    "        ################\n",
    "        # TO COMPLETE\n",
    "        ################\n",
    "    def forward(self, x):\n",
    "        ################\n",
    "        # TO COMPLETE\n",
    "        ################\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set paths and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/epu14/SummerSchool'\n",
    "data_file_name = 'data.npz'\n",
    "nb_iters = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute cardiac metrics\n",
    "\n",
    "First load the data and based on the segmentations compute:\n",
    "\n",
    "- LVEDV (maximum volume of the cardiac cycle)\n",
    "- LVESV (minimum volume of the cardiac cycle)\n",
    "- LVSV = LVEDV - LVESV \n",
    "- LVEF = LVSV/LVEDV*100 \n",
    "\n",
    "Then:\n",
    "- Generate a matrix ```metrics``` that concatenate LVEDV, LVESV, LVSV and LVEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the data\n",
    "data = np.load(os.path.join(data_dir, data_file_name))\n",
    "\n",
    "# 2. Save fields inside data\n",
    "images = data['images']  # (805, 128, 128)\n",
    "segs = data['segs']  # (805, 128, 128)\n",
    "caseNames = data['caseNames']  # 45\n",
    "caseIndices = data['caseIndices']  # 45\n",
    "caseTypeNames = data[\n",
    "    'caseTypeNames']  # ['Normal', 'Heart Failure','Hypertrophy']\n",
    "caseVoxelSize = data['caseVoxelSize']\n",
    "caseTypes = data['caseTypes']  # 45\n",
    "isEDImg = data['isEDImg']  # 805\n",
    "segTypes = data['segTypes']  # ['Background', 'LV Pool', 'Myocardium']\n",
    "\n",
    "# 3. Compute LVEDV, LVESV, LVSV and LVEF and stored in a matrix called metrics \n",
    "\n",
    "################\n",
    "# TO COMPLETE\n",
    "################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the data\n",
    "\n",
    "Plot LVEDV, LVESV, LVSV and LVEF per groups, i.e. LVEDV and LVESV, LVSV and LVEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function PlotThreeClassData to visualize LVEDV vs LVESV and LVSV vs LVEF\n",
    "\n",
    "################\n",
    "# TO COMPLETE\n",
    "################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and test\n",
    "\n",
    "Data is unbalanced (there is no the same number of patients per group). We need to ensure that in the split training/test there is balanced number of classed. To this end use StratifiedShuffleSplit from scikit-learn that will automatically ensure that.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "for train_index, test_index in sss.split(metrics, caseTypes):\n",
    "    X_train, X_test = metrics[train_index, :], metrics[test_index, :]\n",
    "    y_train, y_test = caseTypes[train_index], caseTypes[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a DNN network to perform classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# TO COMPLETE\n",
    "################\n",
    "\n",
    "#1. Instance the DNN network\n",
    "output_size = ...\n",
    "net = DNN(output_size)\n",
    "\n",
    "xt = torch.from_numpy(...).type(torch.FloatTensor)\n",
    "yt = torch.from_numpy(...).type(torch.LongTensor)\n",
    "\n",
    "# Select the loss fuction for classification\n",
    "loss_func = ... \n",
    "\n",
    "# Select the Optimizer with lr = 0.0001\n",
    "opt = ...\n",
    "\n",
    "losses = []\n",
    "\n",
    "for i in range(1, nb_iters + 1):\n",
    "\n",
    "    out = net(xt)  # input x and predict based on x\n",
    "    loss = loss_func(out, yt)   \n",
    "\n",
    "    # clear gradients for next train\n",
    "    ...\n",
    "    # backpropagation, compute gradients\n",
    "    ...\n",
    "     # apply gradients\n",
    "    ...\n",
    "    # Stor the loss over iteration \n",
    "    ...\n",
    "\n",
    "    if i % (nb_iters // 10) == 0:\n",
    "        print(i, losses[-1])\n",
    "\n",
    "# Plot total loss over iterations\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy network and compute metrics\n",
    "\n",
    "First deploy the trained netowrk and then compute the following metrics: balanced accuracy, precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# TO COMPLETE\n",
    "################\n",
    "\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    out = torch.from_numpy(...).float()\n",
    "    # Deploy DNN for the test dataset and compute the predicted label\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "    # Compute metrics\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study 2\n",
    "\n",
    "Redo the same experiment but comparing healthy vs hypertrophic patients and __Healthy__ vs __Hypertrophic cardiomyopathy__ patients \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Case 2: healthy vs Hypertrophy ################# \n",
    "\n",
    "# 1. Generate new labels -- caseTypes2 -- where 0: Non-Hypertrophic and 1:Hypertrophic\n",
    "################\n",
    "# TO COMPLETE\n",
    "################\n",
    "\n",
    "# 2. Plot data using PlotTwoClassData function\n",
    "################\n",
    "# TO COMPLETE\n",
    "################\n",
    "\n",
    "# 3. Split training/test\n",
    "################\n",
    "# TO COMPLETE\n",
    "################\n",
    "\n",
    "# 4. Train DNN\n",
    "################\n",
    "# TO COMPLETE\n",
    "################\n",
    "\n",
    "# 5. Deploy DNN and compute metrics\n",
    "################\n",
    "# TO COMPLETE\n",
    "################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study 3\n",
    "\n",
    "Redo the same experiment but comparing healthy vs hypertrophic patients and __Healthy__ vs __Heart Failure__ patients \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Case 2: healthy vs Heart Failure ################# \n",
    "\n",
    "# 1. Generate new labels -- caseTypes2 -- where 0: Non Heart Failure and 1:Heart Failure\n",
    "################\n",
    "# TO COMPLETE\n",
    "################\n",
    "\n",
    "# 2. Plot data using PlotTwoClassData function\n",
    "################\n",
    "# TO COMPLETE\n",
    "################\n",
    "\n",
    "# 3. Split training/test\n",
    "################\n",
    "# TO COMPLETE\n",
    "################\n",
    "\n",
    "# 4. Train DNN\n",
    "################\n",
    "# TO COMPLETE\n",
    "################\n",
    "\n",
    "# 5. Deploy DNN and compute metrics\n",
    "################\n",
    "# TO COMPLETE\n",
    "################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation parameters \n",
    "\n",
    "Choose the network with higher accuracy and tune the parameters of the DNN network to get the best performances\n",
    "Parameters to modify: \n",
    "- Learning rate\n",
    "- Number of iterations\n",
    "- Activation\n",
    "- Hidden layers\n",
    "- Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (OPTIONAL) Cross validation for test set \n",
    "\n",
    "If we have a single test set, it might not be representative of the data and performance measures can be unreliable. To deal with this we can generate the test set multiple times as well, using k-fold cross-validation. The class ```StratifiedKFold``` splits the data into K folds making sure that labels are represented proportionally. \n",
    "\n",
    "In the code below, input the optimal parameters that you tuned in the previous exercercise. Run the code to see what is the average performance on test sets when using cross-validation strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# TO COMPLETE\n",
    "################\n",
    "\n",
    "# Generate 5 training/test datasets using the StratifiedShuffleSplit function\n",
    "sss = StratifiedShuffleSplit(...)\n",
    "for ... \n",
    "    # For each fold get training and test datasets\n",
    "    ################\n",
    "    # TO COMPLETE\n",
    "    ################\n",
    "    \n",
    "    # Train DNN using the three labels classes: ['Normal', 'Heart Failure','Hypertrophy']\n",
    "    ################\n",
    "    # TO COMPLETE\n",
    "    ################\n",
    "\n",
    "    #  Deploy DNN and compute metrics\n",
    "    ################\n",
    "    # TO COMPLETE\n",
    "    ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (OPTIONAL) Comparison to more standard machine learning techniques: SVM\n",
    "More information about the SVM classifier can be found in: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# TO COMPLETE\n",
    "################\n",
    "# Imports for svm classifier\n",
    "\n",
    "# Generate new labels, i.e. ['non heart failture', 'Heart Failure']\n",
    "\n",
    "# Split training/test\n",
    "\n",
    "# Train and SVM classifier with a rbf kernel\n",
    "\n",
    "# Test\n",
    "\n",
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion/ Questions\n",
    "\n",
    "* which set of clinical parameters performed better in differentiating between groups\n",
    "* How does the learning rate affect to your model?\n",
    "* Which is the influence of choosing different activation functions?\n",
    "* Is there any advantage of using DNN compared to a more standard machine learning techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
